{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and classifying images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import image_transforms\n",
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Audio\n",
    "from datasets import Dataset, load_dataset\n",
    "import librosa\n",
    "from evaluate import load\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = Image.open(\"images/fashion.jpeg\")\n",
    "\n",
    "# Create the numpy array\n",
    "image_array = np.array(original_image)\n",
    "\n",
    "imgplot = plt.imshow(image_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the center of the image\n",
    "cropped_image = image_transforms.center_crop(image=image_array, size=(200, 200))\n",
    "\n",
    "imgplot = plt.imshow(cropped_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_pil_image = Image.fromarray(cropped_image)\n",
    "print(cropped_pil_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "image_classifier = pipeline(task=\"image-classification\", \n",
    "            model=\"abhishek/autotrain_fashion_mnist_vit_base\")\n",
    "\n",
    "# Predict the class of the image\n",
    "results = image_classifier(cropped_pil_image)\n",
    "\n",
    "# Print the results\n",
    "print(results[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What about the original image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the class of the image\n",
    "results = image_classifier(original_image)\n",
    "\n",
    "# Print the results\n",
    "print(results[0][\"label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question answering and multi-modal tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document question and answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "dqa = pipeline(task=\"document-question-answering\", model=\"naver-clova-ix/donut-base-finetuned-docvqa\")\n",
    "\n",
    "# Set the image and question\n",
    "image = \"images/document.jpg\"\n",
    "question = \"Which meeting is this document about?\"\n",
    "\n",
    "# Get the answer\n",
    "results = dqa(image=image, question=question)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual question and answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = \"images/fashion.jpeg\"\n",
    "question = \"What is the model wearing in this image?\"\n",
    "\n",
    "# Create pipeline\n",
    "vqa = pipeline(task=\"visual-question-answering\", model=\"dandelin/vilt-b32-finetuned-vqa\")\n",
    "\n",
    "# Use image and question in vqa\n",
    "results = vqa(image=image, question=question)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"common_language\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.select([0, 1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Save the old sampling rate\n",
    "old_sampling_rate = dataset[1]['audio']['sampling_rate']\n",
    "\n",
    "# Resample the audio files\n",
    "audio_file = dataset.cast_column(\"path\", Audio(sampling_rate=16_000))\n",
    "\n",
    "# Compare the old and new sampling rates\n",
    "print(\"Old sampling rate:\", old_sampling_rate)\n",
    "print(\"New sampling rate:\", dataset[1]['audio']['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict({\"path\": [\"audios/audio1.mp3\", \"audios/audio2.mp3\", \"audios/audio3.mp3\"]}).cast_column(\"path\", Audio())\n",
    "\n",
    "# Create a list of durations\n",
    "old_durations_list = []\n",
    "\n",
    "# Loop over dataset\n",
    "for row in dataset[\"path\"]:\n",
    "    old_durations_list.append(librosa.get_duration(path=row[\"path\"]))\n",
    "\n",
    "# Creat a new column\n",
    "dataset = dataset.add_column(\"duration\", old_durations_list)\n",
    "\n",
    "# Filter the dataset\n",
    "filtered_dataset = dataset.filter(lambda d: d < 60.0, input_columns=[\"duration\"], keep_in_memory=True)\n",
    "\n",
    "# Save new durations\n",
    "new_durations_list = filtered_dataset[\"duration\"]\n",
    "\n",
    "print(\"Old duration:\", np.mean(old_durations_list))\n",
    "print(\"New duration:\", np.mean(new_durations_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"common_language\")\n",
    "dataset = dataset[\"train\"]\n",
    "dataset = dataset.select([0, 1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create the pipeline\n",
    "classifier = pipeline(task=\"audio-classification\", model=\"facebook/mms-lid-126\")\n",
    "\n",
    "# Extract the sample\n",
    "audio = dataset[0]['audio']['array']\n",
    "sentence = dataset[0][\"sentence\"]\n",
    "\n",
    "# Predict the language\n",
    "prediction = classifier(audio)\n",
    "\n",
    "print(f\"Predicted language is '{prediction[0]['label'].upper()}' for the sentence '{sentence}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic speech recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating an ASR pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[6]\n",
    "\n",
    "# Create an ASR pipeline using Meta's wav2vec model\n",
    "meta_asr = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Predict the text from the example audio\n",
    "meta_pred = meta_asr(example[\"audio\"][\"array\"])[\"text\"].lower()\n",
    "\n",
    "# Repeat for OpenAI's Whisper model\n",
    "open_asr = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-tiny\")\n",
    "open_pred = open_asr(example[\"audio\"][\"array\"])[\"text\"].lower()\n",
    "\n",
    "# Print the prediction from both models\n",
    "print(\"META:\", meta_pred)\n",
    "print(\"OPENAI:\", open_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the word error rate metric\n",
    "wer = load(\"wer\")\n",
    "\n",
    "# Save the true sentence of the example\n",
    "true_sentence = example[\"sentence\"].lower()\n",
    "\n",
    "# Compute the wer for each model prediction\n",
    "meta_wer = wer.compute(predictions=[meta_pred], references=[true_sentence])\n",
    "open_wer = wer.compute(predictions=[open_pred], references=[true_sentence])\n",
    "\n",
    "print(f\"The WER for the Meta model is {meta_wer} and for the OpenAI model is {open_wer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data function\n",
    "def data(n=3):\n",
    "    for i in range(n):\n",
    "        yield dataset[i][\"audio\"][\"array\"], dataset[i][\"sentence\"].lower()\n",
    "        \n",
    "# Predict the text for the audio file with both models\n",
    "output = []\n",
    "for audio, sentence in data():\n",
    "    meta_pred = meta_asr(audio)[\"text\"].lower()\n",
    "    open_pred = open_asr(audio)[\"text\"].lower()\n",
    "    # Append to output list\n",
    "    output.append({\"sentence\": sentence, \"metaPred\": meta_pred, \"openPred\": open_pred})\n",
    "\n",
    "output_df = pd.DataFrame(output)\n",
    "\n",
    "# Compute the WER for both models\n",
    "metaWER = wer.compute(predictions=output_df[\"metaPred\"], references=output_df[\"sentence\"])\n",
    "openWER = wer.compute(predictions=output_df[\"openPred\"], references=output_df[\"sentence\"])\n",
    "\n",
    "# Print the WER\n",
    "print(f\"The WER for the meta model is {metaWER} and for the open model is {openWER}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "developing-ai-applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
