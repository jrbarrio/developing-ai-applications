{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, ModelFilter\n",
    "from transformers import AutoModel\n",
    "\n",
    "from datasets import load_dataset_builder, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers and the Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching the Hub with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the instance of the API\n",
    "api = HfApi()\n",
    "\n",
    "# Return the filtered list from the Hub\n",
    "models = api.list_models(\n",
    "    task=\"text-classification\",\n",
    "    sort=\"downloads\",\n",
    "    direction=-1,\n",
    "  \tlimit=1\n",
    ")\n",
    "\n",
    "# Store as a list\n",
    "modelList = list(models)\n",
    "\n",
    "print(modelList[0].modelId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelId = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "\n",
    "# Instantiate the AutoModel class\n",
    "model = AutoModel.from_pretrained(modelId)\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(save_directory=f\"models/{modelId}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset builder\n",
    "reviews_builder = load_dataset_builder(\"derenrich/wikidata-en-descriptions-small\")\n",
    "\n",
    "# Print the features\n",
    "print(reviews_builder.info.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train portion of the dataset\n",
    "wikipedia = load_dataset(\"wikimedia/wikipedia\", \"20231101.en\", split=\"train\")\n",
    "\n",
    "print(f\"The length of the dataset is {len(wikipedia)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the documents\n",
    "filtered = wikipedia.filter(lambda row: \"football\" in row[\"text\"])\n",
    "\n",
    "# Create a sample dataset\n",
    "example = filtered.select(range(1))\n",
    "\n",
    "print(example[0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "developing-ai-applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
