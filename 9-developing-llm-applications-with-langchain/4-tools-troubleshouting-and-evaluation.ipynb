{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizing tools in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool, Tool, create_react_agent, create_structured_chat_agent, AgentExecutor\n",
    "from langchain_openai import OpenAI\n",
    "from langchain import hub\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "from langchain import LLMChain, PromptTemplate\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "import time\n",
    "import os\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the calculate_ltv tool function\n",
    "@tool\n",
    "def calculate_ltv(company_name: str) -> str:\n",
    "    \"\"\"Generate the LTV for a company.\"\"\"\n",
    "    avg_churn = 0.25\n",
    "    avg_revenue = 1000\n",
    "    historical_LTV = avg_revenue / avg_churn\n",
    "\n",
    "    report = f\"LTV Report for {company_name}\\n\"\n",
    "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
    "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
    "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
    "    return report\n",
    "\n",
    "# Define the tools list\n",
    "tools = [Tool(name=\"LTVReport\",\n",
    "              func=calculate_ltv,\n",
    "              description=\"Use this for calculating historical LTV.\")]\n",
    "\n",
    "# Initialize the appropriate agent type\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_react_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, handle_parsing_errors=True, verbose=True, max_iterations=5\n",
    ")\n",
    "\n",
    "input = (\n",
    "    \"Run a financial report that calculates historical LTV for Hooli\"\n",
    ")\n",
    "agent_executor.invoke({\"input\": input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wellness_score(sleep_hours, exercise_minutes, healthy_meals, stress_level):\n",
    "    \"\"\"Calculate a Wellness Score based on sleep, exercise, nutrition, and stress management.\"\"\"\n",
    "    max_score_per_category = 25\n",
    "\n",
    "    sleep_score = min(sleep_hours / 8 * max_score_per_category, max_score_per_category)\n",
    "    exercise_score = min(exercise_minutes / 30 * max_score_per_category, max_score_per_category)\n",
    "    nutrition_score = min(healthy_meals / 3 * max_score_per_category, max_score_per_category)\n",
    "    stress_score = max_score_per_category - min(stress_level / 10 * max_score_per_category, max_score_per_category)\n",
    "\n",
    "    total_score = sleep_score + exercise_score + nutrition_score + stress_score\n",
    "    return total_score\n",
    "\n",
    "# Create a structured tool from calculate_wellness_score()\n",
    "tools = [StructuredTool.from_function(calculate_wellness_score)]\n",
    "\n",
    "# Initialize the appropriate agent type and tool set\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0, openai_api_key=openai_api_key)\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "\n",
    "agent = create_structured_chat_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    prompt=prompt,\n",
    ")\n",
    "\n",
    "wellness_tool = tools[0]\n",
    "result = wellness_tool.func(sleep_hours=8, exercise_minutes=14, healthy_meals=10, stress_level=20)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting tools as OpenAI functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# Create an LTVDescription class to manually add a function description\n",
    "class LTVDescription(BaseModel):\n",
    "    query: str = Field(description='Calculate an extremely simple historical LTV')\n",
    "\n",
    "# Format the calculate_ltv tool function so it can be used by OpenAI models\n",
    "@tool(args_schema=LTVDescription)\n",
    "def calculate_ltv(company_name: str) -> str:\n",
    "    \"\"\"Generate the LTV for a company to pontificate with.\"\"\"\n",
    "    avg_churn = 0.25\n",
    "    avg_revenue = 1000\n",
    "    historical_LTV = avg_revenue / avg_churn\n",
    "\n",
    "    report = f\"Pontification Report for {company_name}\\n\"\n",
    "    report += f\"Avg. churn: ${avg_churn}\\n\"\n",
    "    report += f\"Avg. revenue: ${avg_revenue}\\n\"\n",
    "    report += f\"historical_LTV: ${historical_LTV}\\n\"\n",
    "    return report\n",
    "\n",
    "print(convert_to_openai_function(calculate_ltv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Troubleshouting methods for optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks for troubleshouting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the CallingItIn class to return the prompt, model_name, and temperature\n",
    "class CallingItIn(BaseCallbackHandler):\n",
    "    def on_llm_start(self, serialized, prompts, invocation_params, **kwargs):\n",
    "        print(prompts) \n",
    "        print(invocation_params[\"model_name\"])  \n",
    "        print(invocation_params[\"temperature\"]) \n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", streaming=True, openai_api_key=openai_api_key)\n",
    "prompt_template = \"What do {animal} like to eat?\"\n",
    "prompt=PromptTemplate.from_template(prompt_template)\n",
    "chain = prompt | llm\n",
    "\n",
    "config = {\n",
    "    'callbacks' : [CallingItIn()]\n",
    "}\n",
    "\n",
    "# Call the model with the parameters needed by the prompt\n",
    "output = chain.invoke({\"animal\": \"wombats\"}, config=config)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks for troubleshouting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the PerformanceMonitoringCallback class to return the token and time\n",
    "class PerformanceMonitoringCallback(BaseCallbackHandler):\n",
    "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "    print(f\"Token: {repr(token)} generated at time: {time.time()}\")\n",
    "\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key, temperature=0, streaming=True)\n",
    "prompt_template = \"Describe the process of photosynthesis.\"\n",
    "prompt=PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "config = {\n",
    "    'callbacks' : [PerformanceMonitoringCallback()]\n",
    "}\n",
    "\n",
    "# Call the chain with the callback\n",
    "output = chain.invoke({}, config=config)\n",
    "print(\"Final Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model output in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluator, assign it to criteria, and return result\n",
    "evaluator = load_evaluator(\"criteria\", criteria=\"relevance\", llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "#Â Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    prediction=\"42\",\n",
    "    input=\"What is the answer to the ultimate question of life, the universe, and everything?\",\n",
    ")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluation criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a scalability criterion to custom_criteria\n",
    "custom_criteria = {\n",
    "    \"market_potential\": \"Does the suggestion effectively assess the market potential of the startup?\",\n",
    "    \"innovation\": \"Does the suggestion highlight the startup's innovation and uniqueness in its sector?\",\n",
    "    \"risk_assessment\": \"Does the suggestion provide a thorough analysis of potential risks and mitigation strategies?\",\n",
    "    \"scalability\": \"Does the suggestion address the startup's scalability and growth potential?\"\n",
    "}\n",
    "\n",
    "# Criteria an evaluator from custom_criteria\n",
    "evaluator = load_evaluator(\"criteria\", criteria=custom_criteria, llm=ChatOpenAI(openai_api_key=openai_api_key))\n",
    "\n",
    "# Evaluate the input and prediction\n",
    "eval_result = evaluator.evaluate_strings(\n",
    "    input=\"Should I invest in a startup focused on flying cars? The CEO won't take no for an answer from anyone.\",\n",
    "    prediction=\"No, that is ridiculous.\")\n",
    "\n",
    "print(eval_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.evaluation.qa.eval_chain import QAEvalChain\n",
    "\n",
    "loader = PyPDFLoader('documents/attention-is-all-you-need.pdf')\n",
    "data = loader.load()\n",
    "chunk_size = 200\n",
    "chunk_overlap = 50\n",
    "\n",
    "# Split the quote using RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap)\n",
    "docs = splitter.split_documents(data) \n",
    "\n",
    "question_set = [{'question': 'What is the primary architecture presented in the document?', 'answer': 'The Transformer.'}, {'question': 'According to the document, is the Transformer faster or slower than architectures based on recurrent or convolutional layers?', 'answer': 'The Transformer is faster.'}, {'question': 'Who is the primary author of the document?', 'answer': 'Ashish Vaswani.'}]\n",
    "\n",
    "embedding = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "docstorage = Chroma.from_documents(docs, embedding)\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=docstorage.as_retriever(), input_key=\"question\")\n",
    "\n",
    "# Generate the model responses using the RetrievalQA chain and question_set\n",
    "predictions = qa.batch(question_set)\n",
    "\n",
    "# Define the evaluation chain\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "# Evaluate the ground truth against the answers that are returned\n",
    "results = eval_chain.evaluate(question_set,\n",
    "                              predictions,\n",
    "                              question_key=\"question\",\n",
    "                              prediction_key=\"result\",\n",
    "                              answer_key='answer')\n",
    "\n",
    "for i, q in enumerate(question_set):\n",
    "    print(f\"Question {i+1}: {q['question']}\")\n",
    "    print(f\"Expected Answer: {q['answer']}\")\n",
    "    print(f\"Model Prediction: {predictions[i]['result']}\\n\")\n",
    "    \n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "developing-ai-applications",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
